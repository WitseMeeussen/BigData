{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BigData: architecture catigorization**\n",
    "\n",
    "<h1>DataScraper</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver_manager\n",
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chrome Driver</h3>\n",
    "The raw html data of a google search has a very limited amount of images.\n",
    "\n",
    "That is why we use a chrome driver (from selenium library) to open a chrome browser tab and make a search on google images. Then we tell the driver to scroll down a few times and lastly it gives us the raw html data that is loaded.\n",
    "\n",
    "We have tried programming around the limit that google images sets for scrapers, but we weren't succesfull and it made it a lot slower.\n",
    "\n",
    "Also we run this driver on a larger screen so more images would be loaded per scroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtml(query,scrolls = 10):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    search_url=\"https://www.google.com/search?q={q}&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568\"\n",
    "\n",
    "    driver.get(search_url.format(q=query))\n",
    "\n",
    "    #Scroll to the end of the page\n",
    "    scrollsDone = 0\n",
    "    clicked = False\n",
    "    while scrollsDone<scrolls:\n",
    "        print(scrolls)\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//span[@jsaction= 'h5M12e']\").click()\n",
    "            clicked=True\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print('click failed')\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//input[@jsaction= 'Pmjnye']\").click()\n",
    "            clicked=True\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print('click failed')\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)#sleep_between_interactions\n",
    "        scrollsDone +=1\n",
    "    \n",
    "\n",
    "    #Locate the images to be scraped from the current page \n",
    "    html = driver.page_source\n",
    "    driver.quit\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<input jsaction=\"Pmjnye\" class=\"mye4qd\" type=\"button\" value=\"Show more results\">\n",
    "https://www.google.com/search?q=architecture&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BeautifulSoup</h3>\n",
    "\n",
    "We use BeautifulSoup to read and search the raw html for the right images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImages(html):\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    return soup.find_all(\"img\" ,attrs={\"src\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving the images </h3>\n",
    "\n",
    "This is a loop that loops through the images data and downloads the image via the url in the src atribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveImages(images,location,name = \"\",count=-1):\n",
    "    number = 0\n",
    "    #first image is the google icon\n",
    "    for image in images[1:]:\n",
    "        image_src=image[\"src\"]\n",
    "        \n",
    "        urllib.request.urlretrieve(image_src, location+ name + str(number)+ \".png\")\n",
    "        number += 1\n",
    "        count -=1\n",
    "        if count == 0:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getData is used to to save n images for 1 query in datafolder with name as folder and name for every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(query,n,name,datafolder=\"./Data\"):\n",
    "    images = getImages(getHtml(query,scrolls=round(n/80)+1))\n",
    "    if not os.path.isdir(datafolder+\"/\"+name): os.mkdir(datafolder+\"/\"+name)\n",
    "    saveImages(images,location=datafolder+\"/\"+name+ \"/\", name=name,count=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function n amount of images for every query. 1000 just means max, google images controlles the amount of images available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGoogleImages(querys,n=1000,datafolder=\"./Data\"):\n",
    "    if not os.path.isdir(datafolder): os.mkdir(datafolder)\n",
    "    for q in querys:\n",
    "        getData(query=q,n=n,name=re.sub(\" architecture\",\"\",q),datafolder=datafolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images saved this way are small (100x100 px).\n",
    "This could be an issue if very small details are needed for categorization.\n",
    "But the images are detailed enough to categorize and most categorization models use images from 128x128, not so far of our images.\n",
    "Also this makes scraping the data fast for the amount of pictures, getting higher resolution would require a much longer downloading time.\n",
    "We agreed this was a fair trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"art deco\",\"baroque\",\"gothic\",\"roman\",\"tudor\",\"ancient egyptian\" , \"moorisch\",\"rococo\",\"indoislamic\",\"federal\",\"expressionist\",\"modern\",\"cunstructivist\",\n",
    "querys = [\"art deco\",\"baroque\",\"gothic\",\"roman\",\"tudor\",\"ancient egyptian\" , \"moorisch\",\"rococo\",\"indoislamic\",\"federal\",\"expressionist\",\"modern\",\"cunstructivist\",\"brutalism\"]\n",
    "addArchitecture = lambda q: q + \" architecture\"\n",
    "getGoogleImages(list(map(addArchitecture,querys)),n=20,datafolder=\"./static/data\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7713304168cbd30576c08cd8037e755ec502527db886bcfa010c8949df6f0886"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
